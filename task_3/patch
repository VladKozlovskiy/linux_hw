diff --git a/fs/proc/base.c b/fs/proc/base.c
index dd31e3b..f90d4d0 100644
--- a/fs/proc/base.c
+++ b/fs/proc/base.c
@@ -3246,6 +3246,13 @@ static int proc_stack_depth(struct seq_file *m, struct pid_namespace *ns,
 static const struct file_operations proc_task_operations;
 static const struct inode_operations proc_task_inode_operations;
 
+static int proc_pid_cnt(struct seq_file *m, struct pid_namespace *ns,
+                               struct pid *pid, struct task_struct *task){
+       seq_printf(m,"cnt: %d\n", task->cnt);
+       return 0;
+}
+
+
 static const struct pid_entry tgid_base_stuff[] = {
 	DIR("task",       S_IRUGO|S_IXUGO, proc_task_inode_operations, proc_task_operations),
 	DIR("fd",         S_IRUSR|S_IXUSR, proc_fd_inode_operations, proc_fd_operations),
@@ -3360,6 +3367,8 @@ static const struct pid_entry tgid_base_stuff[] = {
 	ONE("ksm_merging_pages",  S_IRUSR, proc_pid_ksm_merging_pages),
 	ONE("ksm_stat",  S_IRUSR, proc_pid_ksm_stat),
 #endif
+	ONE("cnt",  S_IRUGO, proc_pid_cnt),
+
 };
 
 static int proc_tgid_base_readdir(struct file *file, struct dir_context *ctx)
@@ -3699,6 +3708,7 @@ static const struct pid_entry tid_base_stuff[] = {
 	ONE("ksm_merging_pages",  S_IRUSR, proc_pid_ksm_merging_pages),
 	ONE("ksm_stat",  S_IRUSR, proc_pid_ksm_stat),
 #endif
+	ONE("cnt",  S_IRUGO, proc_pid_cnt),
 };
 
 static int proc_tid_base_readdir(struct file *file, struct dir_context *ctx)
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 292c316..a3d0e63 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1543,6 +1543,8 @@ struct task_struct {
 #ifdef CONFIG_USER_EVENTS
 	struct user_event_mm		*user_event_mm;
 #endif
+	
+	int cnt;
 
 	/*
 	 * New fields for task_struct should be added above here, so that
diff --git a/init/init_task.c b/init/init_task.c
index 5727d42..75eba58 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -210,6 +210,7 @@ struct task_struct init_task
 #ifdef CONFIG_SECCOMP_FILTER
 	.seccomp	= { .filter_count = ATOMIC_INIT(0) },
 #endif
+	.cnt = 0,
 };
 EXPORT_SYMBOL(init_task);
 
diff --git a/kernel/fork.c b/kernel/fork.c
index 10917c3..997ccb8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2730,6 +2730,8 @@ __latent_entropy struct task_struct *copy_process(
 
 	copy_oom_score_adj(clone_flags, p);
 
+	p->cnt = 0;
+
 	return p;
 
 bad_fork_cancel_cgroup:
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index a708d22..212149d 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -3743,6 +3743,7 @@ ttwu_stat(struct task_struct *p, int cpu, int wake_flags)
 static inline void ttwu_do_wakeup(struct task_struct *p)
 {
 	WRITE_ONCE(p->__state, TASK_RUNNING);
+	WRITE_ONCE(p->cnt, READ_ONCE(p->cnt) + 1);
 	trace_sched_wakeup(p);
 }
 
@@ -4793,6 +4794,7 @@ void sched_cgroup_fork(struct task_struct *p, struct kernel_clone_args *kargs)
 	 * required yet, but lockdep gets upset if rules are violated.
 	 */
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
+
 #ifdef CONFIG_CGROUP_SCHED
 	if (1) {
 		struct task_group *tg;
@@ -4848,6 +4850,7 @@ void wake_up_new_task(struct task_struct *p)
 
 	raw_spin_lock_irqsave(&p->pi_lock, rf.flags);
 	WRITE_ONCE(p->__state, TASK_RUNNING);
+	WRITE_ONCE(p->cnt, READ_ONCE(p->cnt) + 1);
 #ifdef CONFIG_SMP
 	/*
 	 * Fork balancing, do it here and not earlier because:
@@ -6619,6 +6622,7 @@ static void __sched notrace __schedule(unsigned int sched_mode)
 	if (!(sched_mode & SM_MASK_PREEMPT) && prev_state) {
 		if (signal_pending_state(prev_state, prev)) {
 			WRITE_ONCE(prev->__state, TASK_RUNNING);
+			WRITE_ONCE(prev->cnt, READ_ONCE(prev->cnt) + 1);
 		} else {
 			prev->sched_contributes_to_load =
 				(prev_state & TASK_UNINTERRUPTIBLE) &&
@@ -8889,6 +8893,7 @@ static inline void preempt_dynamic_init(void) { }
  */
 void __sched yield(void)
 {
+	WRITE_ONCE(current->cnt, READ_ONCE(current->cnt) + 1);
 	set_current_state(TASK_RUNNING);
 	do_sched_yield();
 }
@@ -9226,6 +9231,9 @@ void __init init_idle(struct task_struct *idle, int cpu)
 
 	raw_spin_lock_irqsave(&idle->pi_lock, flags);
 	raw_spin_rq_lock(rq);
+	
+
+	WRITE_ONCE(idle->cnt, READ_ONCE(idle->cnt) + 1);
 
 	idle->__state = TASK_RUNNING;
 	idle->se.exec_start = sched_clock();
